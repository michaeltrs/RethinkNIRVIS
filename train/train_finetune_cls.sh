#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet18_init-rand_LAMP-HQ1of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet18_init-rand_LAMP-HQ2of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet18_init-rand_LAMP-HQ3of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet18_init-rand_LAMP-HQ4of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet18_init-rand_LAMP-HQ5of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet18_init-rand_LAMP-HQ6of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet18_init-rand_LAMP-HQ7of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet18_init-rand_LAMP-HQ8of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet18_init-rand_LAMP-HQ9of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet18_init-rand_LAMP-HQ10of10_w-batchnorm  | tee hist.log

#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet18_init-rand_CASIA2of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet18_init-rand_CASIA3of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet18_init-rand_CASIA4of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet18_init-rand_CASIA5of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet18_init-rand_CASIA6of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1235 train_finetune_cls.py --config iresnet18_init-rand_CASIA7of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1235 train_finetune_cls.py --config iresnet18_init-rand_CASIA8of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1235 train_finetune_cls.py --config iresnet18_init-rand_CASIA9of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1235 train_finetune_cls.py --config iresnet18_init-rand_CASIA10of10_w-batchnorm  | tee hist.log

#----------------------------------------------------------
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.1" --master_port=1236 train_finetune_cls.py --config iresnet18_init-rand_CASIA1of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.1" --master_port=1236 train_finetune_cls.py --config iresnet18_init-rand_OULU-CASIA_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.1" --master_port=1236 train_finetune_cls.py --config iresnet50_init-rand_CASIA1of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.1" --master_port=1236 train_finetune_cls.py --config iresnet50_init-rand_LAMP-HQ1of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.1" --master_port=1236 train_finetune_cls.py --config iresnet50_init-rand_OULU-CASIA_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.1" --master_port=1236 train_finetune_cls.py --config iresnet100_init-rand_CASIA1of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.1" --master_port=1236 train_finetune_cls.py --config iresnet100_init-rand_LAMP-HQ1of10_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.1" --master_port=1236 train_finetune_cls.py --config iresnet100_init-rand_OULU-CASIA_w-batchnorm  | tee hist.log

#----------------------------------------------------------
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet18_init-WF600k_LAMP-HQ1of10_no-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1235 train_finetune_cls.py --config iresnet18_init-WF600k_OULU-CASIA_no-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet50_init-WF600k_LAMP-HQ1of10_no-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet50_init-WF600k_CASIA1of10_no-batchnorm  | tee hist.log

#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.1" --master_port=1236 train_finetune_cls.py --config iresnet18_init-rand_BUAA_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.1" --master_port=1236 train_finetune_cls.py --config iresnet50_init-rand_BUAA_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.1" --master_port=1236 train_finetune_cls.py --config iresnet100_init-rand_BUAA_w-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.1" --master_port=1236 train_finetune_cls.py --config iresnet18_init-WF600k_BUAA_no-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.1" --master_port=1236 train_finetune_cls.py --config iresnet100_init-WF600k_BUAA_no-batchnorm  | tee hist.log

#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet100_init-WF600k_LAMP-HQ1of10_no-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.5" --master_port=1237 train_finetune_cls.py --config iresnet18_init-WF600k_CASIA1of10_no-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.5" --master_port=1237 train_finetune_cls.py --config iresnet100_init-WF600k_CASIA1of10_no-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1235 train_finetune_cls.py --config iresnet100_init-WF600k_OULU-CASIA_no-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.5" --master_port=1237 train_finetune_cls.py --config iresnet50_init-WF600k_OULU-CASIA_no-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.5" --master_port=1237 train_finetune_cls.py --config iresnet50_init-WF600k_BUAA_no-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.5" --master_port=1237 train_finetune_cls.py --config iresnet50_init-WF600k_LAMP-HQ1of10_no-batchnorm  | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.5" --master_port=1237 train_finetune_cls.py --config iresnet50_init-WF600k_CASIA1of10_no-batchnorm  | tee hist.log

#----------------------------------------------------------
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet18_init-WF600k_LAMP-HQ1of10_with-batchnorm | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet50_init-WF600k_LAMP-HQ1of10_with-batchnorm | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet100_init-WF600k_LAMP-HQ1of10_with-batchnorm | tee hist.log
#
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet18_init-WF600k_LAMP-HQ1of10_no-batchnorm_lr1e5 | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet50_init-WF600k_LAMP-HQ1of10_no-batchnorm_lr1e6 | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.2" --master_port=1234 train_finetune_cls.py --config iresnet100_init-WF600k_LAMP-HQ1of10_no-batchnorm_lr1e6 | tee hist.log

#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.3" --master_port=1235 train_finetune_cls.py --config iresnet18_init-WF600k_LAMP-HQ1of10_with-batchnorm_lr1e4 | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.4" --master_port=1236 train_finetune_cls.py --config iresnet18_init-WF600k_LAMP-HQ1of10_no-batchnorm_lr1e4 | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.5" --master_port=1237 train_finetune_cls.py --config iresnet18_init-WF600k_OULU-CASIA_no-batchnorm_lr1e4 | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.6" --master_port=1238 train_finetune_cls.py --config iresnet18_init-WF600k_OULU-CASIA_with-batchnorm_lr1e4 | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.7" --master_port=1239 train_finetune_cls.py --config iresnet18_init-WF600k_BUAA_with-batchnorm_lr1e4 | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.8" --master_port=1240 train_finetune_cls.py --config iresnet18_init-WF600k_CASIA1of10_no-batchnorm_lr1e4 | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.0.9" --master_port=1241 train_finetune_cls.py --config iresnet18_init-WF600k_CASIA1of10_with-batchnorm_lr1e4 | tee hist.log

#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.1.0" --master_port=1242 train_finetune_cls.py --config iresnet18_init-WF600k_BUAA_no-batchnorm_lr1e4 | tee hist.log

#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.1.0" --master_port=1234 train_finetune_cls.py --config iresnet18_init-WF600k_BUAA_with-batchnorm_lr1e5 | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.1.1" --master_port=1235 train_finetune_cls.py --config iresnet18_init-WF600k_CASIA1of10_with-batchnorm_lr1e5 | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.1.2" --master_port=1236 train_finetune_cls.py --config iresnet18_init-WF600k_LAMP-HQ1of10_with-batchnorm_lr1e5 | tee hist.log
#python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.1.3" --master_port=1237 train_finetune_cls.py --config iresnet18_init-WF600k_OULU-CASIA_with-batchnorm_lr1e5 | tee hist.log

python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.1.0" --master_port=1234 train_finetune_cls.py --config iresnet18_init-WF600k_BUAA_no-batchnorm_lr1e5 | tee hist.log
python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.1.1" --master_port=1235 train_finetune_cls.py --config iresnet18_init-WF600k_CASIA1of10_no-batchnorm_lr1e5 | tee hist.log
python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.1.3" --master_port=1237 train_finetune_cls.py --config iresnet18_init-WF600k_OULU-CASIA_no-batchnorm_lr1e5 | tee hist.log

python3 -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr="127.0.1.0" --master_port=1234 train_finetune_cls_RCT_withreg.py | tee hist.log
